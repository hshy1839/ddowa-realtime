'use client';

import { useEffect, useRef, useState } from 'react';

interface WSMessage {
  type: string;
  [key: string]: any;
}

export default function CallPage() {
  const [conversationId, setConversationId] = useState<string>('');
  const [isCallActive, setIsCallActive] = useState(false);
  const [sttText, setSttText] = useState('');
  const [agentText, setAgentText] = useState('');
  const [geminiHealth, setGeminiHealth] = useState<string>('');
  const [wsReady, setWsReady] = useState(false);
  const [micGranted, setMicGranted] = useState(false);
  const [streamingOn, setStreamingOn] = useState(false);
  const [isListening, setIsListening] = useState(false);

  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const conversationIdRef = useRef<string>('');
  const micStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);

  useEffect(() => {
    const wsUrl = process.env.NEXT_PUBLIC_WS_URL || 'ws://localhost:7777';
    console.log(`ğŸ”Œ WebSocket ì—°ê²° ì‹œë„: ${wsUrl}`);
    wsRef.current = new WebSocket(wsUrl);

    wsRef.current.onopen = () => {
      console.log('âœ… WebSocket ì—°ê²°ë¨');
      setWsReady(true);
    };

    wsRef.current.onmessage = (event) => {
      console.log(`ğŸ“¨ WebSocket ë©”ì‹œì§€ ë°›ìŒ:`, event.data);
      const message = JSON.parse(event.data) as WSMessage;

      if (message.type === 'gemini.health') {
        console.log(`ğŸ¥ [GEMINI.HEALTH]`, message);
        setGeminiHealth(message.ok ? 'Gemini OK' : `Gemini FAIL: ${message.message || message.status}`);
        return;
      }

      if (message.type === 'call.started') {
        console.log(`ğŸ“ [CALL.STARTED]`, message.conversationId);
        conversationIdRef.current = message.conversationId;
        setConversationId(message.conversationId);
        setIsCallActive(true);
        startStreaming();
      } else if (message.type === 'stt.delta') {
        console.log(`ğŸ“ [STT.DELTA]`, message.textDelta);
        setSttText((prev) => prev + (message.textDelta || ''));
      } else if (message.type === 'agent.delta') {
        console.log(`ğŸ’¬ [AGENT.DELTA]`, message.textDelta);
        setAgentText((prev) => prev + (message.textDelta || ''));
      } else if (message.type === 'tts.audio') {
        console.log(`ğŸ”Š [TTS.AUDIO] ${message.pcm16ChunkBase64?.length || 0} bytes`);
        playAudio(message.pcm16ChunkBase64);
      } else if (message.type === 'call.ended') {
        console.log(`ğŸ“´ [CALL.ENDED]`);
        setIsCallActive(false);
        stopStreaming();
        console.log('Call ended:', message);
      } else if (message.type === 'error') {
        console.error('âŒ WebSocket error:', message);
      } else {
        console.log(`â“ ì•Œ ìˆ˜ ì—†ëŠ” ë©”ì‹œì§€ íƒ€ì…: ${message.type}`, message);
      }
    };

    wsRef.current.onerror = (error) => {
      console.error('âŒ WebSocket ì—ëŸ¬:', error);
    };

    wsRef.current.onclose = () => {
      console.log('ğŸ”Œ WebSocket ì—°ê²° í•´ì œ');
      setWsReady(false);
      stopStreaming();
    };

    return () => {
      stopStreaming();
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        wsRef.current.close();
      }
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const ensureAudioContext = () => {
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
    }
  };

  const startCall = async () => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.error('âŒ WebSocket not connected');
      alert('WebSocket not connected');
      return;
    }

    console.log('ğŸ¤ ìƒë‹´ ì‹œì‘...');
    ensureAudioContext();

    // Request mic first; streaming begins after call.started arrives
    try {
      console.log('ğŸ™ï¸ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­...');
      micStreamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });
      setMicGranted(true);
      console.log('âœ… ë§ˆì´í¬ ê¶Œí•œ íšë“');
    } catch (error) {
      console.error('âŒ ë§ˆì´í¬ ì ‘ê·¼ ê±°ë¶€:', error);
      setMicGranted(false);
      alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
      return;
    }

    // Reset UI
    setSttText('');
    setAgentText('');
    setGeminiHealth('');
    setStreamingOn(false);

    console.log('ğŸ“¤ call.start ë©”ì‹œì§€ ì „ì†¡');
    wsRef.current.send(JSON.stringify({ type: 'call.start' }));
    console.log('âœ… call.start ì „ì†¡ë¨');
  };

  const stopCall = () => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
    wsRef.current.send(JSON.stringify({ type: 'call.stop', conversationId: conversationIdRef.current }));
  };

  const startStreaming = () => {
    if (!micStreamRef.current) return;
    if (!audioContextRef.current) ensureAudioContext();

    // Avoid duplicate processors
    if (processorRef.current) return;

    const audioContext = audioContextRef.current!;
    const source = audioContext.createMediaStreamSource(micStreamRef.current);
    const processor = audioContext.createScriptProcessor(4096, 1, 1);

    processorRef.current = processor;

    source.connect(processor);
    processor.connect(audioContext.destination);

    setStreamingOn(true);

    processor.onaudioprocess = (event) => {
      const ws = wsRef.current;
      if (!ws || ws.readyState !== WebSocket.OPEN) return;
      if (!conversationIdRef.current) return;

      const inputData = event.inputBuffer.getChannelData(0);
      
      // ë§ˆì´í¬ ì…ë ¥ ìŒëŸ‰ ê°ì§€
      let sum = 0;
      for (let i = 0; i < inputData.length; i++) {
        sum += inputData[i] * inputData[i];
      }
      const rms = Math.sqrt(sum / inputData.length);
      const isVoiceDetected = rms > 0.01; // ì„ê³„ê°’: 0.01
      setIsListening(isVoiceDetected);
      
      const pcm16 = new Int16Array(inputData.length);

      for (let i = 0; i < inputData.length; i++) {
        pcm16[i] = Math.max(-1, Math.min(1, inputData[i])) * 0x7fff;
      }

      const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
      ws.send(
        JSON.stringify({
          type: 'audio.chunk',
          conversationId: conversationIdRef.current,
          pcm16ChunkBase64: base64,
          seq: Date.now(),
          sampleRate: 16000,
        })
      );
    };
  };

  const stopStreaming = () => {
    setStreamingOn(false);

    if (processorRef.current) {
      try {
        processorRef.current.disconnect();
      } catch {}
      processorRef.current.onaudioprocess = null;
      processorRef.current = null;
    }

    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((t) => t.stop());
      micStreamRef.current = null;
    }

    conversationIdRef.current = '';
  };

  const playAudio = (base64: string) => {
    try {
      console.log(`ğŸ”Š [AUDIO PLAY] ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹ : ${base64.length} bytes`);
      
      const binaryString = atob(base64);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      const audioContext = audioContextRef.current || new (window.AudioContext || (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;
      
      console.log(`ğŸ”Š [AUDIO] AudioContext state: ${audioContext.state}`);
      
      // Resume audio context if suspended
      if (audioContext.state === 'suspended') {
        console.warn('âš ï¸ AudioContext suspended - resuming...');
        audioContext.resume().then(() => console.log('âœ… AudioContext resumed'));
      }

      // PCM16 ë°ì´í„°ë¥¼ Float32ë¡œ ë³€í™˜
      const pcm16 = new Int16Array(bytes.buffer);
      const channelData = new Float32Array(pcm16.length);
      for (let i = 0; i < pcm16.length; i++) {
        channelData[i] = pcm16[i] / 32768.0; // Normalize to [-1, 1]
      }

      // OfflineAudioContext ë˜ëŠ” AudioBufferë¡œ ì¬ìƒ
      const audioBuffer = audioContext.createBuffer(1, channelData.length, 16000);
      audioBuffer.copyToChannel(channelData, 0);

      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      
      console.log(`ğŸ”Š [AUDIO] Playing audio buffer: ${channelData.length / 16000}s`);
      source.start();
      console.log(`âœ… [AUDIO] Audio playback started`);
    } catch (error) {
      console.error('âŒ [AUDIO] Error playing audio:', error);
    }
  };

  return (
    <div>
      <h1 className="text-3xl font-bold mb-8">ì‹¤ì‹œê°„ ìƒë‹´</h1>

      <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
        <div className="bg-slate-800 p-6 rounded-lg border border-slate-700">
          <h2 className="text-xl font-bold mb-4">ìƒë‹´ ì œì–´</h2>

          <div className="mb-4">
            <p className="text-slate-400">
              ìƒíƒœ: <span className="font-semibold">{isCallActive ? 'ì§„í–‰ ì¤‘' : 'ëŒ€ê¸° ì¤‘'}</span>
            </p>
            <p className="text-slate-400 text-sm">WebSocket: {wsReady ? 'âœ… ì—°ê²°ë¨' : 'âŒ ì—°ê²° ì•ˆ ë¨'}</p>
            <p className="text-slate-400 text-sm">ë§ˆì´í¬: {micGranted ? 'âœ… í—ˆìš©ë¨' : 'â³ ëŒ€ê¸°/ë¯¸í—ˆìš©'}</p>
            <p className={`text-sm font-semibold ${streamingOn ? 'text-green-400' : 'text-slate-400'}`}>
              ğŸ™ï¸ {streamingOn ? (isListening ? 'âœ¨ ì…ë ¥ ì¤‘...' : 'â¸ï¸ ëŒ€ê¸° ì¤‘') : 'âŒ ë¯¸í™œì„±'}
            </p>
            {conversationId && <p className="text-slate-400 text-sm">ğŸ“ ID: {conversationId.slice(0, 8)}...</p>}
            {geminiHealth && <p className={`text-sm ${geminiHealth.includes('OK') ? 'text-green-400' : 'text-red-400'}`}>ğŸ¥ {geminiHealth}</p>}
          </div>

          <div className="flex gap-4">
            <button
              onClick={startCall}
              disabled={isCallActive}
              className="px-6 py-2 bg-green-600 hover:bg-green-700 disabled:bg-slate-600 rounded font-semibold"
            >
              ìƒë‹´ ì‹œì‘
            </button>
            <button
              onClick={stopCall}
              disabled={!isCallActive}
              className="px-6 py-2 bg-red-600 hover:bg-red-700 disabled:bg-slate-600 rounded font-semibold"
            >
              ìƒë‹´ ì¢…ë£Œ
            </button>
          </div>
        </div>

        <div className="bg-slate-800 p-6 rounded-lg border border-slate-700">
          <h2 className="text-xl font-bold mb-4">ì‹¤ì‹œê°„ ìë§‰</h2>

          <div className="mb-4">
            <p className="text-slate-400 mb-2">ğŸ‘¤ ì‚¬ìš©ì (STT):</p>
            <div className="bg-slate-700 p-4 rounded min-h-[80px] text-sm max-h-[150px] overflow-y-auto">
              {sttText || <span className="text-slate-500">ì…ë ¥ ëŒ€ê¸° ì¤‘...</span>}
            </div>
          </div>

          <div>
            <p className="text-slate-400 mb-2">ğŸ¤– ìƒë‹´ì‚¬ (Agent):</p>
            <div className="bg-blue-900 p-4 rounded min-h-[80px] text-sm max-h-[150px] overflow-y-auto text-blue-100">
              {agentText ? (
                <span>{agentText}</span>
              ) : (
                <span className="text-slate-500">ìƒë‹´ì‚¬ ì‘ë‹µ ëŒ€ê¸° ì¤‘...</span>
              )}
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}
